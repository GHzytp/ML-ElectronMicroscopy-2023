{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SergeiVKalinin/ML-ElectronMicroscopy-2023/blob/main/Lecture%2017/3_0_crVAE_cards.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   This notebook takes you through the application of rotationally invariant conditional variational autoencoder (crVAE) on four different cards datasets and the results analysis.  \n",
        "*   Each cards dataset if formed by applying a random rotation and shear to the images of the cards. The datasets differ in the limits used for picking the rotations and shear.\n",
        "\n",
        "*   conditional autoencoder conditions the encoding-decoding of each image on its ground truth class.\n",
        "*   So, we form m independent latent spaces for a given dataset, where m is the number of classes present in the dataset\n",
        "*   On top of this cVAE (from pyroved) can also be used to condition the encodings on a continuous ground truth property associated with each data point. This is not discussed in this notebook.\n",
        "*   rVAE comprises of an additional latent dimension to the regular VAE, which explicitly captures the rotations involved in the image.\n",
        "*   This angular latent dimension is unsupervisedly learned while trying to reconstuct the original image.\n",
        "*   Like in VAE, this latent dimension is also regularized using a prior.  \n",
        "\n",
        "The crVAEs are defined and trained using a package called Pyroved (https://github.com/ziatdinovmax/pyroVED) which is built on top of Pyro probabilistic programming framework. Please feel free to write to valletisai.mani@gmail.com for further enquiries."
      ],
      "metadata": {
        "id": "Dxjfrq5Rrz3l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mnt4HkaDuYq_",
        "outputId": "2d078046-4068-485f-b3a7-60bd0d9f598e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.1/628.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.7/730.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyroved (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "plotnine 0.10.1 requires matplotlib>=3.5.0, but you have matplotlib 3.3.4 which is incompatible.\n",
            "mizani 0.8.1 requires matplotlib>=3.5.0, but you have matplotlib 3.3.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Installing Kornia and pyroved\n",
        "!pip install -q kornia git+https://github.com/ziatdinovmax/pyroVED@main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary packages\n",
        "\n",
        "import pyroved as pv\n",
        "import kornia as K\n",
        "\n",
        "\n",
        "import cv2\n",
        "from torchvision.utils import make_grid\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['axes.linewidth'] = 3\n",
        "\n",
        "tt = torch.tensor"
      ],
      "metadata": {
        "id": "S5wdpZQqufEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper functions that apply afffine transformations to the dataset\n",
        "rs = lambda x: cv2.resize(x, (64, 64), cv2.INTER_CUBIC)\n",
        "\n",
        "def get_data():\n",
        "    !gdown -q https://drive.google.com/uc?id=1AyGHVflbIjzinkKBURHNVDx1wWg9JixB\n",
        "    !unzip -o -qq cards.zip\n",
        "    card1 = rs(cv2.imread(\"cards/card1.JPG\", cv2.IMREAD_GRAYSCALE))\n",
        "    card2 = rs(cv2.imread(\"cards/card2.JPG\", cv2.IMREAD_GRAYSCALE))\n",
        "    card3 = rs(cv2.imread(\"cards/card3.JPG\", cv2.IMREAD_GRAYSCALE))\n",
        "    card4 = rs(cv2.imread(\"cards/card4.JPG\", cv2.IMREAD_GRAYSCALE))\n",
        "    card1 = tt(1 - card1 / card1.max())\n",
        "    card2 = tt(1 - card2 / card2.max())\n",
        "    card3 = tt(1 - card3 / card3.max())\n",
        "    card4 = tt(1 - card4 / card4.max())\n",
        "    return card1, card2, card3, card4\n",
        "\n",
        "\n",
        "import math\n",
        "def transform_imgs(cards, **kwargs):\n",
        "    np.random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    a = kwargs.get(\"angle\")\n",
        "    t = kwargs.get(\"translation\")\n",
        "    s = (kwargs.get(\"shear\"))\n",
        "    if s is not None:\n",
        "        s = math.radians(s)\n",
        "\n",
        "\n",
        "    n_samples = kwargs.get(\"samples\", 3000)\n",
        "    n_samples_total = n_samples*4\n",
        "\n",
        "    card1, card2, card3, card4 = cards[0], cards[1], cards[2], cards[3]\n",
        "\n",
        "    card1 = card1[None, None].repeat_interleave(n_samples, dim = 0)\n",
        "    card2 = card2[None, None].repeat_interleave(n_samples, dim = 0)\n",
        "    card3 = card3[None, None].repeat_interleave(n_samples, dim = 0)\n",
        "    card4 = card4[None, None].repeat_interleave(n_samples, dim = 0)\n",
        "\n",
        "    cards_all = torch.cat((card1, card2, card3, card4), dim = 0)\n",
        "\n",
        "    angles = tt(np.random.uniform(low = -a, high = a, size = [n_samples_total]))\n",
        "    translations = tt(np.random.uniform(low = -t, high = t, size = [n_samples_total,1])).repeat_interleave(2, dim = 1)\n",
        "    shears = tt(np.random.uniform(low = -s, high = s, size = [n_samples_total, 1])).repeat_interleave(2, dim = 1)\n",
        "\n",
        "    transform = nn.Sequential(\n",
        "        K.augmentation.CenterCrop((48, 48)),\n",
        "        K.geometry.Affine(angle = angles, translation = translations,\n",
        "                          shear = shears)\n",
        "    )\n",
        "\n",
        "    cards = transform(cards_all).squeeze().float()\n",
        "    labels = torch.cat([torch.zeros(len(card1)), torch.ones(len(card2)),\n",
        "                           2*torch.ones(len(card3)), 3*torch.ones(len(card4))])\n",
        "\n",
        "    labels_onehot = pv.utils.to_onehot(labels.to(torch.int64), 4)\n",
        "    return cards, labels_onehot, angles, translations, shears\n",
        "\n",
        "\n",
        "def plot_manifolds(model):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 12),\n",
        "                            subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                            gridspec_kw=dict(hspace=0.1, wspace=0.05))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        manifold = model.manifold2d(d=9, label=i, plot=False)\n",
        "        grid = make_grid(manifold[:, None], nrow=9, pad_value=.5)\n",
        "        ax.imshow(grid[0], cmap='viridis')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Trulxp3Tx9O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the cards dataset\n",
        "!gdown -q https://drive.google.com/uc?id=1AyGHVflbIjzinkKBURHNVDx1wWg9JixB\n",
        "!unzip -o -qq cards.zip"
      ],
      "metadata": {
        "id": "szLTYNGTdztK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we make 4 different datasets using the above helper functions which are as follows:  \n",
        "*  cards-i: low rotations (12 degrees) and low shear (1 degree)\n",
        "*  cards-ii: low rotations (12 degrees) and high shear (20 degrees)\n",
        "*  cards-iii: high rotations (120 degrees) and low shear (1 degree)\n",
        "*  cards-iv: high rotations (120 degrees) and high shear (20 degrees)"
      ],
      "metadata": {
        "id": "Xl_f8wR_d1_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We save the ground truth labels for crVAE\n",
        "# And the ground truth values of the transformations applied to analyze the latent spaces.\n",
        "\n",
        "cards = get_data()\n",
        "cards_all1, labels_all1, angles_all1, translations_all1, shears_all1 = transform_imgs(cards, angle = 12, translation = 0.1, shear = 1, samples = 3000)\n",
        "cards_all2, labels_all2, angles_all2, translations_all2, shears_all2 = transform_imgs(cards, angle = 12, translation = 0.1, shear = 20, samples = 3000)\n",
        "cards_all3, labels_all3, angles_all3, translations_all3, shears_all3 = transform_imgs(cards, angle = 120, translation = 0.1, shear = 1, samples = 3000)\n",
        "cards_all4, labels_all4, angles_all4, translations_all4, shears_all4 = transform_imgs(cards, angle = 120, translation = 0.1, shear = 20, samples = 3000)"
      ],
      "metadata": {
        "id": "vYFgnjhhtQzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the cards before applying transformations\n",
        "fig, ax = plt.subplots(ncols = 4, figsize = (8,2))\n",
        "for i in range(4):\n",
        "    ax[i].imshow(cards[i], cmap = 'gnuplot')\n",
        "    ax[i].axis('off')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "AY5Gx5yg3H9L",
        "outputId": "6bf6fcb2-d826-4c4f-9dfc-5b9f92035855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x200 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "*   Here we train one model of crVAE per dataset and save the weights.\n",
        "*   One can skip this section to save time.\n",
        "*   These weights will be loaded into the notebook in the next section to make it time-efficient"
      ],
      "metadata": {
        "id": "CUusq-ZfePw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Loaders, unlike the rVAE, here the labels are also a part of the trainloader\n",
        "\n",
        "# None corresponds to adding a dummy channel dimension to the dataset\n",
        "train_loader1 = pv.utils.init_dataloader(cards_all1[:, None], labels_all1, batch_size=50)   #cards-i\n",
        "train_loader2 = pv.utils.init_dataloader(cards_all2[:, None], labels_all2, batch_size=50)   #cards-ii\n",
        "train_loader3 = pv.utils.init_dataloader(cards_all3[:, None], labels_all3, batch_size=50)   #cards-iii\n",
        "train_loader4 = pv.utils.init_dataloader(cards_all4[:, None], labels_all4, batch_size=50)   #cards-iv"
      ],
      "metadata": {
        "id": "19itgCEcWnWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the crVAE1 on cards-i\n",
        "\n",
        "in_dim = (48, 48)   # Input dimensions of the dataset i.e., shape of each card's image\n",
        "\n",
        "# Initialize probabilistic VAE model ->\n",
        "# (invariances=None: vanilla VAE\n",
        "#  invariances=['r']: enforces rotational invariance\n",
        "#  invariances=['t']: enforces translational invariance\n",
        "#  invariances=['r', 't']: enforces invariance to rotations & translations\n",
        "#  etc.)\n",
        "crvae1 = pv.models.iVAE(in_dim, latent_dim=2,    # Number of latent dimensions other than the invariancies and class labels\n",
        "                        c_dim = 4,   # Corresponds to the number of discrete classes in the dataset, these get concatenated to the encoder and decoder automatically\n",
        "                      hidden_dim_e = [256,256],   # corresponds to the number of neurons in the hidden layers of the encoder\n",
        "                     hidden_dim_d = [256,256],    # corresponds to the number of neurons in the hidden layers of the decoder\n",
        "                     invariances=['r'], seed=0)\n",
        "\n",
        "# Initialize SVI trainer\n",
        "trainer1 = pv.trainers.SVItrainer(crvae1)\n",
        "# Train for n epochs:\n",
        "for e in range(100):\n",
        "    trainer1.step(train_loader1)\n",
        "    trainer1.print_statistics()\n",
        "\n",
        "# Saving the weights of the trained crVAE1 model on cards-i\n",
        "crvae1.save_weights('/content/drive/MyDrive/VAE Review/crvae_files/crave1')"
      ],
      "metadata": {
        "id": "UMs4q6Lsttmc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c73bd97a-f86b-4032-bf3a-a92ed09885f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Training loss: 532.9758\n",
            "Epoch: 2 Training loss: 235.5862\n",
            "Epoch: 3 Training loss: 181.7076\n",
            "Epoch: 4 Training loss: 141.4826\n",
            "Epoch: 5 Training loss: 124.5735\n",
            "Epoch: 6 Training loss: 114.4570\n",
            "Epoch: 7 Training loss: 107.5991\n",
            "Epoch: 8 Training loss: 102.7164\n",
            "Epoch: 9 Training loss: 99.2995\n",
            "Epoch: 10 Training loss: 95.9591\n",
            "Epoch: 11 Training loss: 94.4574\n",
            "Epoch: 12 Training loss: 94.1248\n",
            "Epoch: 13 Training loss: 91.4784\n",
            "Epoch: 14 Training loss: 89.9675\n",
            "Epoch: 15 Training loss: 90.0478\n",
            "Epoch: 16 Training loss: 88.4633\n",
            "Epoch: 17 Training loss: 87.0561\n",
            "Epoch: 18 Training loss: 86.8243\n",
            "Epoch: 19 Training loss: 85.4625\n",
            "Epoch: 20 Training loss: 85.3742\n",
            "Epoch: 21 Training loss: 85.0257\n",
            "Epoch: 22 Training loss: 84.6261\n",
            "Epoch: 23 Training loss: 85.7041\n",
            "Epoch: 24 Training loss: 83.1370\n",
            "Epoch: 25 Training loss: 83.6706\n",
            "Epoch: 26 Training loss: 84.2428\n",
            "Epoch: 27 Training loss: 82.9594\n",
            "Epoch: 28 Training loss: 82.9336\n",
            "Epoch: 29 Training loss: 81.9363\n",
            "Epoch: 30 Training loss: 81.7262\n",
            "Epoch: 31 Training loss: 82.2002\n",
            "Epoch: 32 Training loss: 81.9048\n",
            "Epoch: 33 Training loss: 84.0448\n",
            "Epoch: 34 Training loss: 81.0800\n",
            "Epoch: 35 Training loss: 80.9185\n",
            "Epoch: 36 Training loss: 80.7761\n",
            "Epoch: 37 Training loss: 81.0214\n",
            "Epoch: 38 Training loss: 81.2333\n",
            "Epoch: 39 Training loss: 80.6177\n",
            "Epoch: 40 Training loss: 81.2996\n",
            "Epoch: 41 Training loss: 80.4720\n",
            "Epoch: 42 Training loss: 80.6064\n",
            "Epoch: 43 Training loss: 80.1832\n",
            "Epoch: 44 Training loss: 80.6199\n",
            "Epoch: 45 Training loss: 80.2473\n",
            "Epoch: 46 Training loss: 80.1532\n",
            "Epoch: 47 Training loss: 79.7184\n",
            "Epoch: 48 Training loss: 80.5919\n",
            "Epoch: 49 Training loss: 79.5140\n",
            "Epoch: 50 Training loss: 79.7563\n",
            "Epoch: 51 Training loss: 80.7549\n",
            "Epoch: 52 Training loss: 79.3087\n",
            "Epoch: 53 Training loss: 80.3072\n",
            "Epoch: 54 Training loss: 79.7223\n",
            "Epoch: 55 Training loss: 79.7212\n",
            "Epoch: 56 Training loss: 79.1909\n",
            "Epoch: 57 Training loss: 79.8646\n",
            "Epoch: 58 Training loss: 80.1214\n",
            "Epoch: 59 Training loss: 79.2451\n",
            "Epoch: 60 Training loss: 79.7370\n",
            "Epoch: 61 Training loss: 78.7556\n",
            "Epoch: 62 Training loss: 79.9218\n",
            "Epoch: 63 Training loss: 78.9272\n",
            "Epoch: 64 Training loss: 79.4819\n",
            "Epoch: 65 Training loss: 78.9106\n",
            "Epoch: 66 Training loss: 79.3146\n",
            "Epoch: 67 Training loss: 79.3965\n",
            "Epoch: 68 Training loss: 79.3093\n",
            "Epoch: 69 Training loss: 78.8317\n",
            "Epoch: 70 Training loss: 79.8122\n",
            "Epoch: 71 Training loss: 78.6968\n",
            "Epoch: 72 Training loss: 79.1996\n",
            "Epoch: 73 Training loss: 78.7370\n",
            "Epoch: 74 Training loss: 78.5336\n",
            "Epoch: 75 Training loss: 78.8531\n",
            "Epoch: 76 Training loss: 79.2352\n",
            "Epoch: 77 Training loss: 79.3993\n",
            "Epoch: 78 Training loss: 78.3480\n",
            "Epoch: 79 Training loss: 78.8517\n",
            "Epoch: 80 Training loss: 78.4072\n",
            "Epoch: 81 Training loss: 78.4548\n",
            "Epoch: 82 Training loss: 78.4111\n",
            "Epoch: 83 Training loss: 79.8076\n",
            "Epoch: 84 Training loss: 78.2898\n",
            "Epoch: 85 Training loss: 78.9043\n",
            "Epoch: 86 Training loss: 78.0370\n",
            "Epoch: 87 Training loss: 78.6066\n",
            "Epoch: 88 Training loss: 78.1963\n",
            "Epoch: 89 Training loss: 78.4435\n",
            "Epoch: 90 Training loss: 78.5048\n",
            "Epoch: 91 Training loss: 77.7888\n",
            "Epoch: 92 Training loss: 78.3735\n",
            "Epoch: 93 Training loss: 78.0804\n",
            "Epoch: 94 Training loss: 78.5166\n",
            "Epoch: 95 Training loss: 78.4781\n",
            "Epoch: 96 Training loss: 78.4158\n",
            "Epoch: 97 Training loss: 78.5546\n",
            "Epoch: 98 Training loss: 77.7414\n",
            "Epoch: 99 Training loss: 77.6384\n",
            "Epoch: 100 Training loss: 78.3799\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-29e3a54e6ed3>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Saving the weights of the trained crVAE1 model on cards-i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mcrvae1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/VAE Review/crvae_files/crave1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyroved/models/base.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;34m\"\"\"Saves trained weights of encoder(s) and decoder.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_writer_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Parent directory /content/drive/MyDrive/VAE Review/crvae_files does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the crVAE2 on cards-ii\n",
        "\n",
        "in_dim = (48, 48)   # Input dimensions of the dataset i.e., shape of each card's image\n",
        "\n",
        "\n",
        "# Initialize probabilistic VAE model ->\n",
        "# (invariances=None: vanilla VAE\n",
        "#  invariances=['r']: enforces rotational invariance\n",
        "#  invariances=['t']: enforces translational invariance\n",
        "#  invariances=['r', 't']: enforces invariance to rotations & translations\n",
        "#  etc.)\n",
        "crvae2 = pv.models.iVAE(in_dim, latent_dim=2,   # Number of latent dimensions other than the invariancies and class labels\n",
        "                        c_dim = 4,   # Corresponds to the number of discrete classes in the dataset, these get concatenated to the encoder and decoder automatically\n",
        "                      hidden_dim_e = [256,256],   # corresponds to the number of neurons in the hidden layers of the encoder\n",
        "                     hidden_dim_d = [256,256],   # corresponds to the number of neurons in the hidden layers of the decoder\n",
        "                     invariances=['r'], seed=0)\n",
        "\n",
        "# Initialize SVI trainer\n",
        "trainer2 = pv.trainers.SVItrainer(crvae2)\n",
        "# Train for n epochs:\n",
        "for e in range(100):\n",
        "    trainer2.step(train_loader2)\n",
        "    trainer2.print_statistics()\n",
        "\n",
        "# Saving the weights of the trained crVAE2 model on cards-ii\n",
        "# crvae2.save_weights('/content/drive/MyDrive/VAE Review/crvae_files/crave2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkPg8LbWt9s9",
        "outputId": "3ffa852f-39d5-4142-9ccc-ed634819e426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Training loss: 588.7591\n",
            "Epoch: 2 Training loss: 328.4248\n",
            "Epoch: 3 Training loss: 248.1311\n",
            "Epoch: 4 Training loss: 203.8312\n",
            "Epoch: 5 Training loss: 170.5263\n",
            "Epoch: 6 Training loss: 148.5587\n",
            "Epoch: 7 Training loss: 135.8697\n",
            "Epoch: 8 Training loss: 133.2234\n",
            "Epoch: 9 Training loss: 123.8817\n",
            "Epoch: 10 Training loss: 124.9711\n",
            "Epoch: 11 Training loss: 117.1617\n",
            "Epoch: 12 Training loss: 117.8607\n",
            "Epoch: 13 Training loss: 111.1267\n",
            "Epoch: 14 Training loss: 112.4652\n",
            "Epoch: 15 Training loss: 107.4927\n",
            "Epoch: 16 Training loss: 114.0145\n",
            "Epoch: 17 Training loss: 103.7819\n",
            "Epoch: 18 Training loss: 105.4050\n",
            "Epoch: 19 Training loss: 102.4699\n",
            "Epoch: 20 Training loss: 102.7087\n",
            "Epoch: 21 Training loss: 104.3618\n",
            "Epoch: 22 Training loss: 100.4642\n",
            "Epoch: 23 Training loss: 100.0843\n",
            "Epoch: 24 Training loss: 103.4682\n",
            "Epoch: 25 Training loss: 98.7285\n",
            "Epoch: 26 Training loss: 99.0776\n",
            "Epoch: 27 Training loss: 100.0739\n",
            "Epoch: 28 Training loss: 97.5046\n",
            "Epoch: 29 Training loss: 98.1350\n",
            "Epoch: 30 Training loss: 100.2412\n",
            "Epoch: 31 Training loss: 97.6043\n",
            "Epoch: 32 Training loss: 96.1696\n",
            "Epoch: 33 Training loss: 98.6759\n",
            "Epoch: 34 Training loss: 98.7914\n",
            "Epoch: 35 Training loss: 95.9348\n",
            "Epoch: 36 Training loss: 96.2119\n",
            "Epoch: 37 Training loss: 95.7640\n",
            "Epoch: 38 Training loss: 95.4854\n",
            "Epoch: 39 Training loss: 97.4814\n",
            "Epoch: 40 Training loss: 94.7851\n",
            "Epoch: 41 Training loss: 95.6319\n",
            "Epoch: 42 Training loss: 94.7459\n",
            "Epoch: 43 Training loss: 96.2497\n",
            "Epoch: 44 Training loss: 96.0312\n",
            "Epoch: 45 Training loss: 94.9554\n",
            "Epoch: 46 Training loss: 95.6878\n",
            "Epoch: 47 Training loss: 93.6191\n",
            "Epoch: 48 Training loss: 92.3620\n",
            "Epoch: 49 Training loss: 95.3294\n",
            "Epoch: 50 Training loss: 93.3887\n",
            "Epoch: 51 Training loss: 93.8511\n",
            "Epoch: 52 Training loss: 96.2096\n",
            "Epoch: 53 Training loss: 93.2686\n",
            "Epoch: 54 Training loss: 94.4840\n",
            "Epoch: 55 Training loss: 93.9892\n",
            "Epoch: 56 Training loss: 92.0171\n",
            "Epoch: 57 Training loss: 92.8375\n",
            "Epoch: 58 Training loss: 92.5490\n",
            "Epoch: 59 Training loss: 92.7822\n",
            "Epoch: 60 Training loss: 92.1538\n",
            "Epoch: 61 Training loss: 91.0428\n",
            "Epoch: 62 Training loss: 93.8915\n",
            "Epoch: 63 Training loss: 93.8473\n",
            "Epoch: 64 Training loss: 92.0152\n",
            "Epoch: 65 Training loss: 90.7474\n",
            "Epoch: 66 Training loss: 91.5220\n",
            "Epoch: 67 Training loss: 93.2087\n",
            "Epoch: 68 Training loss: 90.4324\n",
            "Epoch: 69 Training loss: 91.0391\n",
            "Epoch: 70 Training loss: 90.9112\n",
            "Epoch: 71 Training loss: 91.4206\n",
            "Epoch: 72 Training loss: 91.2426\n",
            "Epoch: 73 Training loss: 91.0847\n",
            "Epoch: 74 Training loss: 90.6172\n",
            "Epoch: 75 Training loss: 92.7147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the crVAE3 on cards-iii\n",
        "in_dim = (48, 48)   # Input dimensions of the dataset i.e., shape of each card's image\n",
        "\n",
        "# Initialize probabilistic VAE model ->\n",
        "# (invariances=None: vanilla VAE\n",
        "#  invariances=['r']: enforces rotational invariance\n",
        "#  invariances=['t']: enforces translational invariance\n",
        "#  invariances=['r', 't']: enforces invariance to rotations & translations\n",
        "#  etc.)\n",
        "crvae3 = pv.models.iVAE(in_dim, latent_dim=2, # Number of latent dimensions other than the invariancies\n",
        "                        c_dim = 4,   # Corresponds to the number of discrete classes in the dataset, these get concatenated to the encoder and decoder automatically\n",
        "                        hidden_dim_e = [256,256],   # corresponds to the number of neurons in the hidden layers of the encoder\n",
        "                        hidden_dim_d = [256,256],   # corresponds to the number of neurons in the hidden layers of the decoder\n",
        "                        invariances=['r'], seed=5)\n",
        "\n",
        "# Initialize SVI trainer\n",
        "trainer3 = pv.trainers.SVItrainer(crvae3)\n",
        "# Train for n epochs:\n",
        "for e in range(100):\n",
        "    trainer3.step(train_loader3)\n",
        "    trainer3.print_statistics()\n",
        "# Saving the weights of the trained crVAE3 model on cards-iii\n",
        "#crvae3.save_weights('/content/drive/MyDrive/VAE Review/crvae_files/crave3')"
      ],
      "metadata": {
        "id": "Chf682NZuGMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the rVAE on cards-iv\n",
        "in_dim = (48, 48)   # Input dimensions of the dataset i.e., shape of each card's image\n",
        "\n",
        "\n",
        "# Initialize probabilistic VAE model ->\n",
        "# (invariances=None: vanilla VAE\n",
        "#  invariances=['r']: enforces rotational invariance\n",
        "#  invariances=['t']: enforces translational invariance\n",
        "#  invariances=['r', 't']: enforces invariance to rotations & translations\n",
        "#  etc.)\n",
        "crvae4 = pv.models.iVAE(in_dim, latent_dim=2, # Number of latent dimensions other than the invariancies\n",
        "                        c_dim = 4,   # Corresponds to the number of discrete classes in the dataset, these get concatenated to the encoder and decoder automatically\n",
        "                        hidden_dim_e = [256,256],   # corresponds to the number of neurons in the hidden layers of the encoder\n",
        "                        hidden_dim_d = [256,256],   # corresponds to the number of neurons in the hidden layers of the decoder\n",
        "                        invariances=['r'], seed=0)\n",
        "\n",
        "# Initialize SVI trainer\n",
        "trainer4 = pv.trainers.SVItrainer(crvae4)\n",
        "# Train for n epochs:\n",
        "for e in range(100):\n",
        "    trainer4.step(train_loader4)\n",
        "    trainer4.print_statistics()\n",
        "# Saving the weights of the trained crVAE4 model on cards-iv\n",
        "# crvae4.save_weights('/content/drive/MyDrive/VAE Review/crvae_files/crave4')"
      ],
      "metadata": {
        "id": "pE9D2UF9uQnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visulizing the results\n",
        "Saved weights will be downloaded in the corresponding sections"
      ],
      "metadata": {
        "id": "WTpSSrWkFHz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cards-i\n",
        "Low rotation (12 deg) and low shear (1 deg)"
      ],
      "metadata": {
        "id": "2tXc4ssrHfZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First let's just visualize some randomly picked samples from the input dataset\n",
        "\n",
        "np.random.seed(1)  # fix seed so that we get the same samples displayed at every run\n",
        "fig, axes = plt.subplots(10, 10, figsize=(8, 8),\n",
        "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
        "\n",
        "for ax in axes.flat:\n",
        "    i = np.random.randint(len(cards_all1))\n",
        "    ax.imshow(cards_all1[i], cmap='gnuplot', interpolation='nearest')"
      ],
      "metadata": {
        "id": "sSAzs8lVEcwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing a new network\n",
        "in_dim = (48,48)\n",
        "crvae1 = pv.models.iVAE(in_dim, latent_dim=2, c_dim = 4,\n",
        "                      hidden_dim_e = [256,256],\n",
        "                     hidden_dim_d = [256,256],\n",
        "                     invariances=['r'], seed=1)\n",
        "# Download the weigths file\n",
        "!gdown 1h2BRSRhT21e5R-AO1c5MeJYLKgNhMs-Y\n",
        "\n",
        "# Loading the saved weights into the newly initialized network\n",
        "crvae1.load_weights('crvae1.pt')"
      ],
      "metadata": {
        "id": "VArFwCRMFM2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoded latent space:\n",
        "# The latent space is uniformly sampled in z1-z2 latent dimensions.\n",
        "# These points are decoded back into the image space and are plotted in their corresponding latent space positions\n",
        "# Since the latent space of crVAE is conditioned on the class label, we get one latent space for each class\n",
        "\n",
        "for cls in range(4):\n",
        "    cls = pv.utils.to_onehot(tt([cls,]), 4)\n",
        "    crvae1.manifold2d(d=9, y=cls, cmap=\"gnuplot\")   # d corresponds to the number of points sampled in each latent dimension"
      ],
      "metadata": {
        "id": "oatwb5JEFW1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Latent Distributions:  \n",
        "Input images are represented as points in the latent space using the encoder  \n",
        "These points are then colored with various ground-truth properties to visualize their distribution"
      ],
      "metadata": {
        "id": "382KxqSrhdkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "targets_all1 = torch.argmax(labels_all1, dim = 1)  # Obtaining the classes from one-hot encoded vectors\n",
        "z_mean1, z_sd1 = crvae1.encode(cards_all1[:, None], labels_all1)# Encoding the full input dataset\n",
        "# z_mean1: mean of the latent space representation of the input dataset,\n",
        "# z_sd1: standard deviation of the latent space representation of the input dataset,\n",
        "# 0th column corresponds to angle and then the regular latent dimensions (First invariant dimensions and then regular latent dimensions)\n",
        "\n",
        "\n",
        "# Latent distribution colored using the suit of each image\n",
        "fig, ax = plt.subplots(1, 1, figsize =(7, 6))\n",
        "im = ax.scatter(z_mean1[:,-1], z_mean1[:,-2], c=targets_all1, s=1, cmap='jet')\n",
        "ax.set_xlabel(\"$z_1$\", fontsize=14)\n",
        "ax.set_ylabel(\"$z_2$\", fontsize=14)\n",
        "cbar = fig.colorbar(im, ax=ax, shrink=.8)\n",
        "cbar.set_label(\"Label\", fontsize=14)\n",
        "cbar.ax.tick_params(labelsize=10)\n",
        "plt.show()\n",
        "\n",
        "# It should be noted that each suit (color) lives in an independent latent space from the other suits (colors)"
      ],
      "metadata": {
        "id": "PItZ2e87FgaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the latent space of each suit is independent, we plot the latent distributions in four different latent spaces corresponding to their suit"
      ],
      "metadata": {
        "id": "nO_4PfHMiRZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Latent distribution colored using the ground truth shear applied on each image\n",
        "\n",
        "fig, ax = plt.subplots(nrows = 2, ncols = 2, figsize = (7,7))\n",
        "\n",
        "inds = np.zeros([len(targets_all1), 4], dtype = bool)\n",
        "for i in range(4):\n",
        "    inds[:, i] = np.asarray(targets_all1 == i, dtype = bool)\n",
        "\n",
        "ax[0,0].scatter(z_mean1[inds[:,0], -1], z_mean1[inds[:,0],-2], c=shears_all1[inds[:,0],0], s=1, cmap='jet')\n",
        "ax[0,0].set_xlabel(\"$z_1$\", fontsize=14)\n",
        "ax[0,0].set_ylabel(\"$z_2$\", fontsize=14)\n",
        "\n",
        "ax[0,1].scatter(z_mean1[inds[:,1], -1], z_mean1[inds[:,1],-2], c=shears_all1[inds[:,1],0], s=1, cmap='jet')\n",
        "ax[0,1].set_xlabel(\"$z_1$\", fontsize=14)\n",
        "ax[0,1].set_ylabel(\"$z_2$\", fontsize=14)\n",
        "\n",
        "ax[1,0].scatter(z_mean1[inds[:,2], -1], z_mean1[inds[:,2],-2], c=shears_all1[inds[:,2],0], s=1, cmap='jet')\n",
        "ax[1,0].set_xlabel(\"$z_1$\", fontsize=14)\n",
        "ax[1,0].set_ylabel(\"$z_2$\", fontsize=14)\n",
        "\n",
        "ax[1,1].scatter(z_mean1[inds[:,3], -1], z_mean1[inds[:,3],-2], c=shears_all1[inds[:,3],0], s=1, cmap='jet')\n",
        "ax[1,1].set_xlabel(\"$z_1$\", fontsize=14)\n",
        "ax[1,1].set_ylabel(\"$z_2$\", fontsize=14)\n",
        "\n"
      ],
      "metadata": {
        "id": "lfnNfMUQGBP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally the plot of encoded angle vs. ground truth rotation applied to the images.\n",
        "# The points are colored using the suit of the images.\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (7,7))\n",
        "ax.scatter(torch.deg2rad(angles_all1), z_mean1[:,0], c = targets_all1, s =1, cmap = 'jet')"
      ],
      "metadata": {
        "id": "uUPa1H60F7bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cards-ii\n",
        "Low rotation (12 deg) High shear (20deg)"
      ],
      "metadata": {
        "id": "ovLC7izWHZ03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First let's just visualize some randomly picked samples from the input dataset\n",
        "\n",
        "np.random.seed(1)  # fix seed so that we get the same samples displayed at every run\n",
        "fig, axes = plt.subplots(10, 10, figsize=(8, 8),\n",
        "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
        "\n",
        "for ax in axes.flat:\n",
        "    i = np.random.randint(len(cards_all2))\n",
        "    ax.imshow(cards_all2[i], cmap='gnuplot', interpolation='nearest')"
      ],
      "metadata": {
        "id": "KurE5C8BHkUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing a new network\n",
        "in_dim = (48, 48)\n",
        "crvae2 = pv.models.iVAE(in_dim, latent_dim=2, c_dim = 4,\n",
        "                      hidden_dim_e = [256,256],\n",
        "                     hidden_dim_d = [256,256],\n",
        "                     invariances=['r'], seed=1)\n",
        "\n",
        "# Download the weigths file\n",
        "!gdown 1--OtYXcmrGaIaMZNQIVj-QjevZcW80CD\n",
        "\n",
        "# Loading the saved weights into the newly initialized network\n",
        "crvae2.load_weights('crvae2.pt')"
      ],
      "metadata": {
        "id": "pPtjm_80HqG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoded latent space:\n",
        "# The latent space is uniformly sampled in z1-z2 latent dimensions.\n",
        "# These points are decoded back into the image space and are plotted in their corresponding latent space positions\n",
        "# Since the latent space of crVAE is conditioned on the class label, we get one latent space for each class\n",
        "\n",
        "for cls in range(4):\n",
        "    cls = pv.utils.to_onehot(tt([cls,]), 4)\n",
        "    crvae2.manifold2d(d=9, y=cls, cmap=\"viridis\")  # d corresponds to the number of points sampled in each latent dimension"
      ],
      "metadata": {
        "id": "1NNAik7xHwal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Latent Distributions:  \n",
        "Input images are represented as points in the latent space using the encoder  \n",
        "These points are then colored with various ground-truth properties to visualize their distribution"
      ],
      "metadata": {
        "id": "mQBgHtUFkSCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "targets_all2 = torch.argmax(labels_all2, dim = 1)   # Obtaining the classes from one-hot encoded vectors\n",
        "z_mean2, z_sd2 = crvae2.encode(cards_all2[:, None], labels_all2)   # Encoding the full input dataset\n",
        "# z_mean1: mean of the latent space representation of the input dataset,\n",
        "# z_sd1: standard deviation of the latent space representation of the input dataset,\n",
        "# 0th column corresponds to angle and then the regular latent dimensions (First invariant dimensions and then regular latent dimensions)\n",
        "\n",
        "# Latent distribution colored using the suit of each image\n",
        "fig, ax = plt.subplots(1, 1, figsize =(7, 6))\n",
        "im = ax.scatter(z_mean2[:,-1], z_mean2[:,-2], c=targets_all2, s=1, cmap='jet')\n",
        "ax.set_xlabel(\"$z_1$\", fontsize=14)\n",
        "ax.set_ylabel(\"$z_2$\", fontsize=14)\n",
        "cbar = fig.colorbar(im, ax=ax, shrink=.8)\n",
        "cbar.set_label(\"Label\", fontsize=14)\n",
        "cbar.ax.tick_params(labelsize=10)\n",
        "plt.show()\n",
        "\n",
        "# It should be noted that each suit (color) lives in an independent latent space from the other suits (colors)"
      ],
      "metadata": {
        "id": "USkYX5SJH1Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the latent space of each suit is independent, we plot the latent distributions in four different latent spaces corresponding to their suit"
      ],
      "metadata": {
        "id": "lZa8d8vwlUu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Latent distribution colored using the ground-truth shear of each image\n",
        "\n",
        "fig, ax = plt.subplots(nrows = 2, ncols = 2, figsize = (7,7))\n",
        "\n",
        "inds = np.zeros([len(targets_all2), 4], dtype = bool)\n",
        "for i in range(4):\n",
        "    inds[:, i] = np.asarray(targets_all2 == i, dtype = bool)\n",
        "\n",
        "ax[0,0].scatter(z_mean2[inds[:,0], -1], z_mean2[inds[:,0],-2], c=shears_all2[inds[:,0],0], s=1, cmap='jet')\n",
        "ax[0,0].set_xlabel(\"$z_1$\", fontsize=14)\n",
        "ax[0,0].set_ylabel(\"$z_2$\", fontsize=14)\n",
        "\n",
        "ax[0,1].scatter(z_mean2[inds[:,1], -1], z_mean2[inds[:,1],-2], c=shears_all2[inds[:,1],0], s=1, cmap='jet')\n",
        "ax[0,1].set_xlabel(\"$z_1$\", fontsize=14)\n",
        "ax[0,1].set_ylabel(\"$z_2$\", fontsize=14)\n",
        "\n",
        "ax[1,0].scatter(z_mean2[inds[:,2], -1], z_mean2[inds[:,2],-2], c=shears_all2[inds[:,2],0], s=1, cmap='jet')\n",
        "ax[1,0].set_xlabel(\"$z_1$\", fontsize=14)\n",
        "ax[1,0].set_ylabel(\"$z_2$\", fontsize=14)\n",
        "\n",
        "ax[1,1].scatter(z_mean2[inds[:,3], -1], z_mean2[inds[:,3],-2], c=shears_all2[inds[:,3],0], s=1, cmap='jet')\n",
        "ax[1,1].set_xlabel(\"$z_1$\", fontsize=14)\n",
        "ax[1,1].set_ylabel(\"$z_2$\", fontsize=14)"
      ],
      "metadata": {
        "id": "-2RW9yBQH85k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally the plot of encoded angle vs. ground truth rotation applied to the images.\n",
        "# The points are colored using the suit of the images.\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (7,7))\n",
        "ax.scatter(torch.deg2rad(angles_all2), z_mean2[:,0], c = targets_all2, s =1, cmap = 'jet')"
      ],
      "metadata": {
        "id": "FUe_N5adIX3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cards-iii\n",
        "High rotation (120 degrees) and Low shear (1 degree)"
      ],
      "metadata": {
        "id": "miQkMoxmIcze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First let's just visualize some randomly picked samples from the input dataset\n",
        "\n",
        "np.random.seed(1)  # fix seed so that we get the same samples displayed at every run\n",
        "fig, axes = plt.subplots(10, 10, figsize=(8, 8),\n",
        "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
        "\n",
        "for ax in axes.flat:\n",
        "    i = np.random.randint(len(cards_all3))\n",
        "    ax.imshow(cards_all3[i], cmap='gnuplot', interpolation='nearest')"
      ],
      "metadata": {
        "id": "_bF8a-bbIfws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing a new network\n",
        "in_dim = (48,48)\n",
        "crvae3 = pv.models.iVAE(in_dim, latent_dim=2, c_dim = 4,\n",
        "                      hidden_dim_e = [256,256],\n",
        "                     hidden_dim_d = [256,256],\n",
        "                     invariances=['r'], seed=1)\n",
        "# Download the weigths file\n",
        "!gdown 1-FSNB55Wwo7T6qeh-gFjAdvjEUu38coq\n",
        "\n",
        "# Loading the saved weights into the newly initialized network\n",
        "crvae3.load_weights('crvae3.pt')"
      ],
      "metadata": {
        "id": "JaV9piJHIoxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoded latent space:\n",
        "# The latent space is uniformly sampled in z1-z2 latent dimensions.\n",
        "# These points are decoded back into the image space and are plotted in their corresponding latent space positions\n",
        "# Since the latent space of crVAE is conditioned on the class label, we get one latent space for each class\n",
        "\n",
        "for cls in range(4):\n",
        "    cls = pv.utils.to_onehot(tt([cls,]), 4)\n",
        "    crvae3.manifold2d(d=9, y=cls, cmap=\"viridis\")"
      ],
      "metadata": {
        "id": "D4l829i6IueF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Latent Distributions:  \n",
        "Input images are represented as points in the latent space using the encoder  \n",
        "These points are then colored with various ground-truth properties to visualize their distribution"
      ],
      "metadata": {
        "id": "XX6RgXe0kUGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "targets_all3 = torch.argmax(labels_all3, dim = 1)   # Obtaining the classes from one-hot encoded vectors\n",
        "z_mean3, z_sd3 = crvae3.encode(cards_all3[:, None], labels_all3)   # Encoding the full input dataset\n",
        "# z_mean1: mean of the latent space representation of the input dataset,\n",
        "# z_sd1: standard deviation of the latent space representation of the input dataset,\n",
        "# 0th column corresponds to angle and then the regular latent dimensions (First invariant dimensions and then regular latent dimensions)\n",
        "\n",
        "\n",
        "# Latent distribution colored using the suit of each image\n",
        "fig, ax = plt.subplots(1, 1, figsize =(7, 6))\n",
        "im = ax.scatter(z_mean3[:,-1], z_mean3[:,-2], c=targets_all3, s=1, cmap='jet')\n",
        "ax.set_xlabel(\"$z_1$\", fontsize=14)\n",
        "ax.set_ylabel(\"$z_2$\", fontsize=14)\n",
        "cbar = fig.colorbar(im, ax=ax, shrink=.8)\n",
        "cbar.set_label(\"Label\", fontsize=14)\n",
        "cbar.ax.tick_params(labelsize=10)\n",
        "plt.show()\n",
        "# It should be noted that each suit (color) lives in an independent latent space from the other suits (colors)"
      ],
      "metadata": {
        "id": "5aLjWRZ6Izcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the latent space of each suit is independent, we plot the latent distributions in four different latent spaces corresponding to their suit"
      ],
      "metadata": {
        "id": "lYEkMNPclXID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Latent distribution colored using the ground-truth shear of each image\n",
        "fig, ax = plt.subplots(nrows = 2, ncols = 2, figsize = (7,7))\n",
        "\n",
        "inds = np.zeros([len(targets_all3), 4], dtype = bool)\n",
        "for i in range(4):\n",
        "    inds[:, i] = np.asarray(targets_all3 == i, dtype = bool)\n",
        "\n",
        "ax[0,0].scatter(z_mean3[inds[:,0], -1], z_mean3[inds[:,0],-2], c=shears_all3[inds[:,0],0], s=1, cmap='jet')\n",
        "ax[0,0].set_xlabel(\"$z_1$\", fontsize=14)\n",
        "ax[0,0].set_ylabel(\"$z_2$\", fontsize=14)\n",
        "\n",
        "ax[0,1].scatter(z_mean3[inds[:,1], -1], z_mean3[inds[:,1],-2], c=shears_all3[inds[:,1],0], s=1, cmap='jet')\n",
        "ax[0,1].set_xlabel(\"$z_1$\", fontsize=14)\n",
        "ax[0,1].set_ylabel(\"$z_2$\", fontsize=14)\n",
        "\n",
        "ax[1,0].scatter(z_mean3[inds[:,2], -1], z_mean3[inds[:,2],-2], c=shears_all3[inds[:,2],0], s=1, cmap='jet')\n",
        "ax[1,0].set_xlabel(\"$z_1$\", fontsize=14)\n",
        "ax[1,0].set_ylabel(\"$z_2$\", fontsize=14)\n",
        "\n",
        "ax[1,1].scatter(z_mean3[inds[:,3], -1], z_mean3[inds[:,3],-2], c=shears_all3[inds[:,3],0], s=1, cmap='jet')\n",
        "ax[1,1].set_xlabel(\"$z_1$\", fontsize=14)\n",
        "ax[1,1].set_ylabel(\"$z_2$\", fontsize=14)"
      ],
      "metadata": {
        "id": "YNocQ7d9I-Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally the plot of encoded angle vs. ground truth rotation applied to the images.\n",
        "# The points are colored using the suit of the images.\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (7,7))\n",
        "ax.scatter(torch.deg2rad(angles_all3), z_mean3[:,0], c = targets_all3, s =25, cmap = 'jet')\n",
        "ax.set_xlabel(\"ground truth\", fontsize=14)\n",
        "ax.set_ylabel(\"angular encoding\", fontsize=14)"
      ],
      "metadata": {
        "id": "1WMRId91JSuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cards-iv\n",
        "High rotation (120 degrees) and high shear (20 degrees)"
      ],
      "metadata": {
        "id": "jmdut772JWjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First let's just visualize some randomly picked samples from the input dataset\n",
        "\n",
        "np.random.seed(1)  # fix seed so that we get the same samples displayed at every run\n",
        "fig, axes = plt.subplots(10, 10, figsize=(8, 8),\n",
        "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
        "\n",
        "for ax in axes.flat:\n",
        "    i = np.random.randint(len(cards_all4))\n",
        "    ax.imshow(cards_all4[i], cmap='gnuplot', interpolation='nearest')"
      ],
      "metadata": {
        "id": "SXGgHceOJbWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing a new network\n",
        "in_dim = (48,48)\n",
        "crvae4 = pv.models.iVAE(in_dim, latent_dim=2, c_dim = 4,\n",
        "                      hidden_dim_e = [256,256],\n",
        "                     hidden_dim_d = [256,256],\n",
        "                     invariances=['r'], seed=1)\n",
        "\n",
        "# Download the weigths file\n",
        "!gdown 1-LeIFSUxBWRgUZMgCnrlaDW8fqlkN_E6\n",
        "\n",
        "# Loading the saved weights into the newly initialized network\n",
        "crvae4.load_weights('crvae4.pt')"
      ],
      "metadata": {
        "id": "Ga3e8-_eJiNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoded latent space:\n",
        "# The latent space is uniformly sampled in z1-z2 latent dimensions.\n",
        "# These points are decoded back into the image space and are plotted in their corresponding latent space positions\n",
        "# Since the latent space of crVAE is conditioned on the class label, we get one latent space for each class\n",
        "\n",
        "for cls in range(4):\n",
        "    cls = pv.utils.to_onehot(tt([cls,]), 4)\n",
        "    crvae4.manifold2d(d=9, y=cls, cmap=\"viridis\")   # d corresponds to the number of points sampled in each latent dimension"
      ],
      "metadata": {
        "id": "5Ma4eknvJnNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Latent Distributions:  \n",
        "Input images are represented as points in the latent space using the encoder  \n",
        "These points are then colored with various ground-truth properties to visualize their distribution"
      ],
      "metadata": {
        "id": "_vdp61SAkWB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "targets_all4 = torch.argmax(labels_all4, dim = 1)   # Obtaining the classes from one-hot encoded vectors\n",
        "z_mean4, z_sd4 = crvae4.encode(cards_all4[:, None], labels_all4)   # Encoding the full input dataset\n",
        "# z_mean1: mean of the latent space representation of the input dataset,\n",
        "# z_sd1: standard deviation of the latent space representation of the input dataset,\n",
        "# 0th column corresponds to angle and then the regular latent dimensions (First invariant dimensions and then regular latent dimensions)\n",
        "\n",
        "\n",
        "# Latent distribution colored using the suit of each image\n",
        "fig, ax = plt.subplots(1, 1, figsize =(5, 5))\n",
        "im = ax.scatter(z_mean4[:,-1], z_mean4[:,-2], c=targets_all4, s=10, cmap='jet')\n",
        "ax.set_xlabel(\"$z_1$\", fontsize=14)\n",
        "ax.set_ylabel(\"$z_2$\", fontsize=14)\n",
        "cbar = fig.colorbar(im, ax=ax, shrink=.8)\n",
        "cbar.set_label(\"Label\", fontsize=14)\n",
        "cbar.ax.tick_params(labelsize=10)\n",
        "\n",
        "# plt.savefig('crvae_case4_latent_dis.png', dpi = 300)\n",
        "plt.show()\n",
        "\n",
        "# It should be noted that each suit (color) lives in an independent latent space from the other suits (colors)"
      ],
      "metadata": {
        "id": "yiugheEbJrce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the latent space of each suit is independent, we plot the latent distributions in four different latent spaces corresponding to their suit"
      ],
      "metadata": {
        "id": "dFw2-kVMlbLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Latent distribution colored using the ground truth shear of each image\n",
        "fig, ax = plt.subplots(nrows = 2, ncols = 2, figsize = (7,7))\n",
        "\n",
        "inds = np.zeros([len(targets_all4), 4], dtype = bool)\n",
        "for i in range(4):\n",
        "    inds[:, i] = np.asarray(targets_all4 == i, dtype = bool)\n",
        "\n",
        "ax[0,0].scatter(z_mean4[inds[:,0], -1], z_mean4[inds[:,0],-2], c=shears_all4[inds[:,0],0], s=1, cmap='jet')\n",
        "ax[0,0].set_xlabel(\"$z_1$\", fontsize=14)\n",
        "ax[0,0].set_ylabel(\"$z_2$\", fontsize=14)\n",
        "\n",
        "ax[0,1].scatter(z_mean4[inds[:,1], -1], z_mean4[inds[:,1],-2], c=shears_all4[inds[:,1],0], s=1, cmap='jet')\n",
        "ax[0,1].set_xlabel(\"$z_1$\", fontsize=14)\n",
        "ax[0,1].set_ylabel(\"$z_2$\", fontsize=14)\n",
        "\n",
        "ax[1,0].scatter(z_mean4[inds[:,2], -1], z_mean4[inds[:,2],-2], c=shears_all4[inds[:,2],0], s=1, cmap='jet')\n",
        "ax[1,0].set_xlabel(\"$z_1$\", fontsize=14)\n",
        "ax[1,0].set_ylabel(\"$z_2$\", fontsize=14)\n",
        "\n",
        "ax[1,1].scatter(z_mean4[inds[:,3], -1], z_mean4[inds[:,3],-2], c=shears_all4[inds[:,3],0], s=1, cmap='jet')\n",
        "ax[1,1].set_xlabel(\"$z_1$\", fontsize=14)\n",
        "ax[1,1].set_ylabel(\"$z_2$\", fontsize=14)"
      ],
      "metadata": {
        "id": "xgD2KdDRJ1Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally the plot of encoded angle vs. ground truth rotation applied to the images.\n",
        "# The points are colored using the suit of the images.\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (5,5))\n",
        "ax.scatter(torch.deg2rad(angles_all4), z_mean4[:,0], c = targets_all4, s =25, cmap = 'jet')\n",
        "ax.set_xlabel(\"ground truth\", fontsize=14)\n",
        "ax.set_ylabel(\"angular encoding\", fontsize=14)\n",
        "# plt.savefig('crvae_case4_angle_plot.png', dpi = 300)"
      ],
      "metadata": {
        "id": "IkkgEaqYKFOs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}